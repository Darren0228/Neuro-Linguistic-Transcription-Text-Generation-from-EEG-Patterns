{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import:\n",
    "\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mne.preprocessing import ICA\n",
    "import pywt\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "mne.set_log_level('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Just One File to Test:\n",
    "\n",
    "file_participant_1 = 'Data/RawData/Participant_1.edf'\n",
    "\n",
    "edf_data_files = [\n",
    "    file_participant_1]\n",
    "\n",
    "channels_of_interest = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "\n",
    "participant_1_COI_dataset = mne.io.read_raw_edf(file_participant_1, preload = True).pick_channels(channels_of_interest)\n",
    "\n",
    "# Function to Preprocess Raw Data:\n",
    "\n",
    "def preprocess_raw_data(raw):\n",
    "\n",
    "    # Handling NaNs: Replace NaNs with the mean of the respective channel\n",
    "    raw_data = raw.get_data()\n",
    "\n",
    "    for i in range(raw_data.shape[0]):\n",
    "        nan_indices = np.isnan(raw_data[i])\n",
    "\n",
    "        if np.any(nan_indices):\n",
    "            mean_value = np.nanmean(raw_data[i])\n",
    "            raw_data[i, nan_indices] = mean_value\n",
    "\n",
    "    raw._data = raw_data\n",
    "\n",
    "    # Filtering: Bandpass filter between 4-30 Hz\n",
    "    raw.filter(4., 30., fir_design='firwin')\n",
    "    \n",
    "    # Artifact Removal: ICA\n",
    "    ica = mne.preprocessing.ICA(n_components = 14, random_state = 97, max_iter = 800)\n",
    "    ica.fit(raw)\n",
    "    raw = ica.apply(raw)\n",
    "    \n",
    "    # Spatial Filtering: Common Average Reference (CAR)\n",
    "    raw.set_eeg_reference('average', projection = True)\n",
    "    \n",
    "    # Channel Interpolation: Interpolate bad channels\n",
    "    raw.interpolate_bads()\n",
    "\n",
    "    # Baseline Correction: Apply baseline correction using the mean of the segment\n",
    "    raw.apply_function(lambda x: x - np.mean(x), picks = 'eeg')\n",
    "    \n",
    "    return raw\n",
    "\n",
    "participant_1_preprocessed_dataset = preprocess_raw_data(participant_1_COI_dataset)\n",
    "\n",
    "# Function to Extract Segments:\n",
    "\n",
    "def extract_segment(raw, start_sec, duration_sec, label, sfreq):\n",
    "\n",
    "    start_sample = int(start_sec * sfreq)\n",
    "    end_sample = start_sample + int(duration_sec * sfreq)\n",
    "    segment = raw[:, start_sample:end_sample][0]\n",
    "    \n",
    "    return segment, label\n",
    "\n",
    "# Segmentation Details:\n",
    "\n",
    "segments = [\n",
    "    (30, 60, \"I\"),\n",
    "    (120, 60, \"Yes\"),\n",
    "    (210, 60, \"No\"),\n",
    "    (300, 60, \"Want\"),\n",
    "    (390, 60, \"Help\"),\n",
    "    (480, 60, \"More\"),\n",
    "    (570, 60, \"That\"),\n",
    "    (660, 60, \"Stop\"),\n",
    "    (750, 60, \"Open\"),\n",
    "    (840, 60, \"Close\")\n",
    "]\n",
    "\n",
    "# Extract segments for Participant 1:\n",
    "sfreq_participant_1 = participant_1_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_1 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_1_preprocessed_dataset, start, duration, label, sfreq_participant_1)\n",
    "    segments_participant_1.append((segment, label))\n",
    "\n",
    "    # Function to Create Sub-Epochs from Segments:\n",
    "\n",
    "def create_sub_epochs(segment, epoch_duration, sfreq):\n",
    "\n",
    "    n_samples_per_epoch = int(epoch_duration * sfreq)\n",
    "    n_epochs = segment.shape[1] // n_samples_per_epoch\n",
    "    sub_epochs = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        start_sample = i * n_samples_per_epoch\n",
    "        end_sample = start_sample + n_samples_per_epoch\n",
    "        sub_epoch = segment[:, start_sample:end_sample]\n",
    "        sub_epochs.append(sub_epoch)\n",
    "    \n",
    "    return sub_epochs\n",
    "\n",
    "sub_epoch_duration = 2\n",
    "# Create sub-epochs for Participant 1\n",
    "sub_epochs_participant_1 = []\n",
    "for segment, label in segments_participant_1:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_1)\n",
    "    sub_epochs_participant_1.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "data_sub_epochs_participant_1 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_1], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_1.to_csv('Data/SubEpochData/Participant_1_Sub_Epoch_Data.csv', index = False)\n",
    "\n",
    "concatenated_sub_epoch_dataset = pd.concat([data_sub_epochs_participant_1 \n",
    "                                            ]).reset_index(drop = True)\n",
    "\n",
    "concatenated_sub_epoch_dataset.to_csv('Data/FinalDataset/Combined_Sub_Epoch_Final_Dataset.csv', index = False)\n",
    "\n",
    "# Function to Extract Power Spectral Density (PSD) Features:\n",
    "\n",
    "def extract_psd_features(epochs, bands, sfreq):\n",
    "\n",
    "    psd_features = []\n",
    "\n",
    "    for epoch, label in epochs:\n",
    "        psd, freqs = mne.time_frequency.psd_array_multitaper(epoch, sfreq = sfreq, fmin = 0.5, fmax = 30)\n",
    "        band_powers = {'Label': label}\n",
    "\n",
    "        for band, (low, high) in bands.items():\n",
    "            band_power = np.mean(psd[:, (freqs >= low) & (freqs <= high)], axis = 1)\n",
    "\n",
    "            for i, power in enumerate(band_power):\n",
    "                band_powers[f'{band}_ch{i}'] = power\n",
    "\n",
    "        psd_features.append(band_powers)\n",
    "\n",
    "    return pd.DataFrame(psd_features)\n",
    "\n",
    "\n",
    "def extract_wavelet_features(epochs, wavelet = 'db4', level = 5):\n",
    "\n",
    "    wavelet_features = []\n",
    "\n",
    "    for epoch, label in epochs:\n",
    "        features = {'Label': label}\n",
    "\n",
    "        for ch in range(epoch.shape[0]):\n",
    "            coeffs = pywt.wavedec(epoch[ch], wavelet, level = level)\n",
    "\n",
    "            for i, coeff in enumerate(coeffs):\n",
    "                features[f'ch{ch}_coeff{i}_mean'] = np.mean(coeff)\n",
    "                features[f'ch{ch}_coeff{i}_std'] = np.std(coeff)\n",
    "\n",
    "        wavelet_features.append(features)\n",
    "\n",
    "    return pd.DataFrame(wavelet_features)\n",
    "\n",
    "# Defining Frequency bands for PSD:\n",
    "\n",
    "bands = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 12),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (31, 50)\n",
    "}\n",
    "\n",
    "# Combine all participants' sub-epochs:\n",
    "all_sub_epochs = (sub_epochs_participant_1)\n",
    "\n",
    "# Extract PSD features:\n",
    "psd_features = extract_psd_features(all_sub_epochs, bands, sfreq_participant_1)\n",
    "\n",
    "# Extract Wavelet features:\n",
    "wavelet_features = extract_wavelet_features(all_sub_epochs)\n",
    "\n",
    "# Combine features:\n",
    "combined_features = pd.merge(psd_features, wavelet_features, on = 'Label')\n",
    "\n",
    "# Save combined features to CSV\n",
    "combined_features.to_csv('Data/FinalDataset/Full_Final_Dataset_Test_1_Participant.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining File Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing EDF Files:\n",
    "\n",
    "file_participant_1 = 'Data/RawData/Participant_1.edf'\n",
    "file_participant_2 = 'Data/RawData/Participant_2.edf'\n",
    "file_participant_3 = 'Data/RawData/Participant_3.edf'\n",
    "file_participant_4 = 'Data/RawData/Participant_4.edf'\n",
    "file_participant_5 = 'Data/RawData/Participant_5.edf'\n",
    "file_participant_6 = 'Data/RawData/Participant_6.edf'\n",
    "file_participant_7 = 'Data/RawData/Participant_7.edf'\n",
    "file_participant_8 = 'Data/RawData/Participant_8.edf'\n",
    "file_participant_9 = 'Data/RawData/Participant_9.edf'\n",
    "file_participant_10 = 'Data/RawData/Participant_10.edf'\n",
    "file_participant_11 = 'Data/RawData/Participant_11.edf'\n",
    "file_participant_12 = 'Data/RawData/Participant_12.edf'\n",
    "file_participant_13 = 'Data/RawData/Participant_13.edf'\n",
    "\n",
    "edf_data_files = [\n",
    "    file_participant_1,\n",
    "    file_participant_2,\n",
    "    file_participant_3,\n",
    "    file_participant_4,\n",
    "    file_participant_5,\n",
    "    file_participant_6,\n",
    "    file_participant_7,\n",
    "    file_participant_8,\n",
    "    file_participant_9,\n",
    "    file_participant_10,\n",
    "    file_participant_11,\n",
    "    file_participant_12,\n",
    "    file_participant_13\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Channels of Interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COI:\n",
    "\n",
    "channels_of_interest = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the COI:\n",
    "\n",
    "participant_1_COI_dataset = mne.io.read_raw_edf(file_participant_1, preload = True).pick_channels(channels_of_interest)\n",
    "participant_2_COI_dataset = mne.io.read_raw_edf(file_participant_2, preload = True).pick_channels(channels_of_interest)\n",
    "participant_3_COI_dataset = mne.io.read_raw_edf(file_participant_3, preload = True).pick_channels(channels_of_interest)\n",
    "participant_4_COI_dataset = mne.io.read_raw_edf(file_participant_4, preload = True).pick_channels(channels_of_interest)\n",
    "participant_5_COI_dataset = mne.io.read_raw_edf(file_participant_5, preload = True).pick_channels(channels_of_interest)\n",
    "participant_6_COI_dataset = mne.io.read_raw_edf(file_participant_6, preload = True).pick_channels(channels_of_interest)\n",
    "participant_7_COI_dataset = mne.io.read_raw_edf(file_participant_7, preload = True).pick_channels(channels_of_interest)\n",
    "participant_8_COI_dataset = mne.io.read_raw_edf(file_participant_8, preload = True).pick_channels(channels_of_interest)\n",
    "participant_9_COI_dataset = mne.io.read_raw_edf(file_participant_9, preload = True).pick_channels(channels_of_interest)\n",
    "participant_10_COI_dataset = mne.io.read_raw_edf(file_participant_10, preload = True).pick_channels(channels_of_interest)\n",
    "participant_11_COI_dataset = mne.io.read_raw_edf(file_participant_11, preload = True).pick_channels(channels_of_interest)\n",
    "participant_12_COI_dataset = mne.io.read_raw_edf(file_participant_12, preload = True).pick_channels(channels_of_interest)\n",
    "participant_13_COI_dataset = mne.io.read_raw_edf(file_participant_13, preload = True).pick_channels(channels_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting Raw EDF Info:\n",
    "\n",
    "print(participant_1_COI_dataset)\n",
    "print(participant_2_COI_dataset)\n",
    "print(participant_3_COI_dataset)\n",
    "print(participant_4_COI_dataset)\n",
    "print(participant_5_COI_dataset)\n",
    "print(participant_6_COI_dataset)\n",
    "print(participant_7_COI_dataset)\n",
    "print(participant_8_COI_dataset)\n",
    "print(participant_9_COI_dataset)\n",
    "print(participant_10_COI_dataset)\n",
    "print(participant_11_COI_dataset)\n",
    "print(participant_12_COI_dataset)\n",
    "print(participant_13_COI_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess Raw Data:\n",
    "\n",
    "def preprocess_raw_data(raw):\n",
    "\n",
    "    # Handling NaNs: Replace NaNs with the mean of the respective channel\n",
    "    raw_data = raw.get_data()\n",
    "\n",
    "    for i in range(raw_data.shape[0]):\n",
    "        nan_indices = np.isnan(raw_data[i])\n",
    "\n",
    "        if np.any(nan_indices):\n",
    "            mean_value = np.nanmean(raw_data[i])\n",
    "            raw_data[i, nan_indices] = mean_value\n",
    "\n",
    "    raw._data = raw_data\n",
    "\n",
    "    # Filtering: Bandpass filter between 4-30 Hz\n",
    "    raw.filter(0.5, 30., fir_design='firwin')\n",
    "    \n",
    "    # Artifact Removal: ICA\n",
    "    ica = mne.preprocessing.ICA(n_components = 14, random_state = 97, max_iter = 800)\n",
    "    ica.fit(raw)\n",
    "    raw = ica.apply(raw)\n",
    "    \n",
    "    # Spatial Filtering: Common Average Reference (CAR)\n",
    "    raw.set_eeg_reference('average', projection = True)\n",
    "    \n",
    "    # Channel Interpolation: Interpolate bad channels\n",
    "    raw.interpolate_bads()\n",
    "\n",
    "    # Baseline Correction: Apply baseline correction using the mean of the segment\n",
    "    raw.apply_function(lambda x: x - np.mean(x), picks = 'eeg')\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the Preprocessing Function:\n",
    "\n",
    "participant_1_preprocessed_dataset = preprocess_raw_data(participant_1_COI_dataset)\n",
    "participant_2_preprocessed_dataset = preprocess_raw_data(participant_2_COI_dataset)\n",
    "participant_3_preprocessed_dataset = preprocess_raw_data(participant_3_COI_dataset)\n",
    "participant_4_preprocessed_dataset = preprocess_raw_data(participant_4_COI_dataset)\n",
    "participant_5_preprocessed_dataset = preprocess_raw_data(participant_5_COI_dataset)\n",
    "participant_6_preprocessed_dataset = preprocess_raw_data(participant_6_COI_dataset)\n",
    "participant_7_preprocessed_dataset = preprocess_raw_data(participant_7_COI_dataset)\n",
    "participant_8_preprocessed_dataset = preprocess_raw_data(participant_8_COI_dataset)\n",
    "participant_9_preprocessed_dataset = preprocess_raw_data(participant_9_COI_dataset)\n",
    "participant_10_preprocessed_dataset = preprocess_raw_data(participant_10_COI_dataset)\n",
    "participant_11_preprocessed_dataset = preprocess_raw_data(participant_11_COI_dataset)\n",
    "participant_12_preprocessed_dataset = preprocess_raw_data(participant_12_COI_dataset)\n",
    "participant_13_preprocessed_dataset = preprocess_raw_data(participant_13_COI_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segmentation of Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation occurs as following:\n",
    "\n",
    "# First 30 secs -> removed since these serve as the adjustment period\n",
    "# 60 secs -> \"I\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"Yes\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"No\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"Want\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"Help\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"More\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"That\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"Stop\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"Open\"\n",
    "# 30 secs -> Break\n",
    "# 60 secs -> \"Close\"\n",
    "# Remaining time remove as it is redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Segments:\n",
    "\n",
    "def extract_segment(raw, start_sec, duration_sec, label, sfreq):\n",
    "\n",
    "    start_sample = int(start_sec * sfreq)\n",
    "    end_sample = start_sample + int(duration_sec * sfreq)\n",
    "    segment = raw[:, start_sample:end_sample][0]\n",
    "    \n",
    "    return segment, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation Details:\n",
    "\n",
    "segments = [\n",
    "    (30, 60, \"I\"),\n",
    "    (120, 60, \"Yes\"),\n",
    "    (210, 60, \"No\"),\n",
    "    (300, 60, \"Want\"),\n",
    "    (390, 60, \"Help\"),\n",
    "    (480, 60, \"More\"),\n",
    "    (570, 60, \"That\"),\n",
    "    (660, 60, \"Stop\"),\n",
    "    (750, 60, \"Open\"),\n",
    "    (840, 60, \"Close\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Segments for each Individual Participant Part 1:\n",
    "\n",
    "# Extract segments for Participant 1:\n",
    "sfreq_participant_1 = participant_1_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_1 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_1_preprocessed_dataset, start, duration, label, sfreq_participant_1)\n",
    "    segments_participant_1.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 2:\n",
    "sfreq_participant_2 = participant_2_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_2 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_2_preprocessed_dataset, start, duration, label, sfreq_participant_2)\n",
    "    segments_participant_2.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 3:\n",
    "sfreq_participant_3 = participant_3_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_3 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_3_preprocessed_dataset, start, duration, label, sfreq_participant_3)\n",
    "    segments_participant_3.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 4:\n",
    "sfreq_participant_4 = participant_4_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_4 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_4_preprocessed_dataset, start, duration, label, sfreq_participant_4)\n",
    "    segments_participant_4.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 5:\n",
    "sfreq_participant_5 = participant_5_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_5 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_5_preprocessed_dataset, start, duration, label, sfreq_participant_5)\n",
    "    segments_participant_5.append((segment, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Segments for each Individual Participant Part 2:\n",
    "\n",
    "# Extract segments for Participant 6:\n",
    "sfreq_participant_6 = participant_6_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_6 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_6_preprocessed_dataset, start, duration, label, sfreq_participant_6)\n",
    "    segments_participant_6.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 7:\n",
    "sfreq_participant_7 = participant_7_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_7 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_7_preprocessed_dataset, start, duration, label, sfreq_participant_7)\n",
    "    segments_participant_7.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 8:\n",
    "sfreq_participant_8 = participant_8_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_8 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_8_preprocessed_dataset, start, duration, label, sfreq_participant_8)\n",
    "    segments_participant_8.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 9:\n",
    "sfreq_participant_9 = participant_9_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_9 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_9_preprocessed_dataset, start, duration, label, sfreq_participant_9)\n",
    "    segments_participant_9.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 10:\n",
    "sfreq_participant_10 = participant_10_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_10 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_10_preprocessed_dataset, start, duration, label, sfreq_participant_10)\n",
    "    segments_participant_10.append((segment, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Segments for each Individual Participant Part 3:\n",
    "\n",
    "# Extract segments for Participant 11:\n",
    "sfreq_participant_11 = participant_11_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_11 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_11_preprocessed_dataset, start, duration, label, sfreq_participant_11)\n",
    "    segments_participant_11.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 12:\n",
    "sfreq_participant_12 = participant_12_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_12 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_12_preprocessed_dataset, start, duration, label, sfreq_participant_12)\n",
    "    segments_participant_12.append((segment, label))\n",
    "\n",
    "# Extract segments for Participant 13:\n",
    "sfreq_participant_13 = participant_13_preprocessed_dataset.info['sfreq']\n",
    "segments_participant_13 = []\n",
    "\n",
    "for start, duration, label in segments:\n",
    "    segment, label = extract_segment(participant_13_preprocessed_dataset, start, duration, label, sfreq_participant_13)\n",
    "    segments_participant_13.append((segment, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispalying Segments for Participant 1:\n",
    "\n",
    "segments_participant_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Sub Epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create Sub-Epochs from Segments:\n",
    "\n",
    "def create_sub_epochs(segment, epoch_duration, sfreq):\n",
    "\n",
    "    n_samples_per_epoch = int(epoch_duration * sfreq)\n",
    "    n_epochs = segment.shape[1] // n_samples_per_epoch\n",
    "    sub_epochs = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        start_sample = i * n_samples_per_epoch\n",
    "        end_sample = start_sample + n_samples_per_epoch\n",
    "        sub_epoch = segment[:, start_sample:end_sample]\n",
    "        sub_epochs.append(sub_epoch)\n",
    "    \n",
    "    return sub_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Sub-Epoch Duration:\n",
    "\n",
    "sub_epoch_duration = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Sub-Epochs for each Individual Participant Part 1:\n",
    "\n",
    "# Create sub-epochs for Participant 1\n",
    "sub_epochs_participant_1 = []\n",
    "for segment, label in segments_participant_1:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_1)\n",
    "    sub_epochs_participant_1.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 2\n",
    "sub_epochs_participant_2 = []\n",
    "for segment, label in segments_participant_2:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_2)\n",
    "    sub_epochs_participant_2.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 3\n",
    "sub_epochs_participant_3 = []\n",
    "for segment, label in segments_participant_3:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_3)\n",
    "    sub_epochs_participant_3.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 4\n",
    "sub_epochs_participant_4 = []\n",
    "for segment, label in segments_participant_4:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_4)\n",
    "    sub_epochs_participant_4.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 5\n",
    "sub_epochs_participant_5 = []\n",
    "for segment, label in segments_participant_5:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_5)\n",
    "    sub_epochs_participant_5.extend([(sub_epoch, label) for sub_epoch in sub_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Sub-Epochs for each Individual Participant Part 2:\n",
    "\n",
    "# Create sub-epochs for Participant 6\n",
    "sub_epochs_participant_6 = []\n",
    "for segment, label in segments_participant_6:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_6)\n",
    "    sub_epochs_participant_6.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 7\n",
    "sub_epochs_participant_7 = []\n",
    "for segment, label in segments_participant_7:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_7)\n",
    "    sub_epochs_participant_7.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 8\n",
    "sub_epochs_participant_8 = []\n",
    "for segment, label in segments_participant_8:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_8)\n",
    "    sub_epochs_participant_8.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 9\n",
    "sub_epochs_participant_9 = []\n",
    "for segment, label in segments_participant_9:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_9)\n",
    "    sub_epochs_participant_9.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 10\n",
    "sub_epochs_participant_10 = []\n",
    "for segment, label in segments_participant_10:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_10)\n",
    "    sub_epochs_participant_10.extend([(sub_epoch, label) for sub_epoch in sub_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Sub-Epochs for each Individual Participant Part 3:\n",
    "\n",
    "# Create sub-epochs for Participant 11\n",
    "sub_epochs_participant_11 = []\n",
    "for segment, label in segments_participant_11:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_11)\n",
    "    sub_epochs_participant_11.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 12\n",
    "sub_epochs_participant_12 = []\n",
    "for segment, label in segments_participant_12:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_12)\n",
    "    sub_epochs_participant_12.extend([(sub_epoch, label) for sub_epoch in sub_epochs])\n",
    "\n",
    "# Create sub-epochs for Participant 13\n",
    "sub_epochs_participant_13 = []\n",
    "for segment, label in segments_participant_13:\n",
    "    sub_epochs = create_sub_epochs(segment, sub_epoch_duration, sfreq_participant_13)\n",
    "    sub_epochs_participant_13.extend([(sub_epoch, label) for sub_epoch in sub_epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Sub-Epoch Data into DataFrames:\n",
    "\n",
    "data_sub_epochs_participant_1 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_1], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_2 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_2], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_3 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_3], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_4 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_4], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_5 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_5], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_6 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_6], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_7 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_7], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_8 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_8], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_9 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_9], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_10 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_10], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_11 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_11], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_12 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_12], columns=['Label', 'Sub_Epoch'])\n",
    "data_sub_epochs_participant_13 = pd.DataFrame([(label, sub_epoch) for sub_epoch, label in sub_epochs_participant_13], columns=['Label', 'Sub_Epoch'])\n",
    "\n",
    "\n",
    "# Save Sub-Epoch Data to CSV files for Verification:\n",
    "\n",
    "data_sub_epochs_participant_1.to_csv('Data/SubEpochData/Participant_1_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_2.to_csv('Data/SubEpochData/Participant_2_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_3.to_csv('Data/SubEpochData/Participant_3_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_4.to_csv('Data/SubEpochData/Participant_4_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_5.to_csv('Data/SubEpochData/Participant_5_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_6.to_csv('Data/SubEpochData/Participant_6_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_7.to_csv('Data/SubEpochData/Participant_7_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_8.to_csv('Data/SubEpochData/Participant_8_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_9.to_csv('Data/SubEpochData/Participant_9_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_10.to_csv('Data/SubEpochData/Participant_10_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_11.to_csv('Data/SubEpochData/Participant_11_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_12.to_csv('Data/SubEpochData/Participant_12_Sub_Epoch_Data.csv', index = False)\n",
    "data_sub_epochs_participant_13.to_csv('Data/SubEpochData/Participant_13_Sub_Epoch_Data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Sub-Epoch Data for Verification:\n",
    "\n",
    "print(\"Participant 1 Sub-Epoch Data:\")\n",
    "print(data_sub_epochs_participant_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constructing Final Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Sub-Epoch Data from all Participants into a Single Dataset:\n",
    "\n",
    "concatenated_sub_epoch_dataset = pd.concat([data_sub_epochs_participant_1, \n",
    "                                            data_sub_epochs_participant_2, \n",
    "                                            data_sub_epochs_participant_3,\n",
    "                                            data_sub_epochs_participant_4,\n",
    "                                            data_sub_epochs_participant_5,\n",
    "                                            data_sub_epochs_participant_6,\n",
    "                                            data_sub_epochs_participant_7,\n",
    "                                            data_sub_epochs_participant_8,\n",
    "                                            data_sub_epochs_participant_9,\n",
    "                                            data_sub_epochs_participant_10,\n",
    "                                            data_sub_epochs_participant_11,\n",
    "                                            data_sub_epochs_participant_12,\n",
    "                                            data_sub_epochs_participant_13]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the First few rows to ensure the Concatenation was Successful:\n",
    "print(\"Combined Sub-Epoch Data (first few rows):\")\n",
    "print(concatenated_sub_epoch_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Combined Dataset to CSV:\n",
    "\n",
    "concatenated_sub_epoch_dataset.to_csv('Data/FinalDataset/Combined_Sub_Epoch_Final_Dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Power Spectral Density (PSD) Features:\n",
    "\n",
    "def extract_psd_features(epochs, bands, sfreq):\n",
    "\n",
    "    psd_features = []\n",
    "    logging.info(f\"Extracting PSD Features for {len(epochs)} epochs.\")\n",
    "\n",
    "    for epoch, label in epochs:\n",
    "        psd, freqs = mne.time_frequency.psd_array_multitaper(epoch, sfreq = sfreq, fmin = 0.5, fmax = 30)\n",
    "        band_powers = {'Label': label}\n",
    "\n",
    "        for band, (low, high) in bands.items():\n",
    "            band_power = np.mean(psd[:, (freqs >= low) & (freqs <= high)], axis = 1)\n",
    "\n",
    "            for i, power in enumerate(band_power):\n",
    "                band_powers[f'{band}_ch{i}'] = power\n",
    "\n",
    "        psd_features.append(band_powers)\n",
    "\n",
    "    logging.info(f\"\\nPSD Features Extracted: {psd_features[0].keys()}\")\n",
    "    return pd.DataFrame(psd_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract Wavelet Transform Features:\n",
    "\n",
    "def extract_wavelet_features(epochs, wavelet = 'db4', level = 5):\n",
    "\n",
    "    wavelet_features = []\n",
    "    logging.info(f\"Extracting Wavelet Features for {len(epochs)} epochs.\")\n",
    "\n",
    "    for epoch, label in epochs:\n",
    "        features = {'Label': label}\n",
    "\n",
    "        for ch in range(epoch.shape[0]):\n",
    "            coeffs = pywt.wavedec(epoch[ch], wavelet, level = level)\n",
    "\n",
    "            for i, coeff in enumerate(coeffs):\n",
    "                features[f'ch{ch}_coeff{i}_mean'] = np.mean(coeff)\n",
    "                features[f'ch{ch}_coeff{i}_std'] = np.std(coeff)\n",
    "\n",
    "        wavelet_features.append(features)\n",
    "\n",
    "    logging.info(f\"\\nWavelet Features Extracted: {wavelet_features[0].keys()}\")\n",
    "    return pd.DataFrame(wavelet_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Frequency bands for PSD:\n",
    "\n",
    "bands = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 12),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (31, 50)\n",
    "}\n",
    "\n",
    "# Combine all participants' sub-epochs:\n",
    "all_sub_epochs = (sub_epochs_participant_1 + sub_epochs_participant_2 + sub_epochs_participant_3 + sub_epochs_participant_4 + sub_epochs_participant_5 + sub_epochs_participant_6 + sub_epochs_participant_7 + sub_epochs_participant_8 + sub_epochs_participant_9 + sub_epochs_participant_10 + sub_epochs_participant_11 + sub_epochs_participant_12 + sub_epochs_participant_13)\n",
    "\n",
    "# Extract PSD Features:\n",
    "psd_features = extract_psd_features(all_sub_epochs, bands, sfreq_participant_1)\n",
    "logging.info(f\"\\nPSD features DataFrame shape: {psd_features.shape}\")\n",
    "\n",
    "# Extract Wavelet Features:\n",
    "wavelet_features = extract_wavelet_features(all_sub_epochs)\n",
    "logging.info(f\"\\nWavelet features DataFrame Shape: {wavelet_features.shape}\")\n",
    "\n",
    "# Combine Features:\n",
    "combined_features = pd.merge(psd_features, wavelet_features, on = 'Label')\n",
    "logging.info(f\"\\nCombined features DataFrame shape: {combined_features.shape}\")\n",
    "\n",
    "# Drop Duplicate Features:\n",
    "combined_features = combined_features.loc[:,~combined_features.columns.duplicated()]\n",
    "logging.info(f\"\\nCombined features DataFrame shape after dropping duplicates: {combined_features.shape}\")\n",
    "\n",
    "# Save combined features to CSV\n",
    "combined_features.to_csv('Data/FinalDataset/Full_Final_Dataset.csv', index = False)\n",
    "logging.info(f\"\\nFinal combined features saved. Number of features: {combined_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Extracted Features:\n",
    "\n",
    "print(\"Extracted Features:\")\n",
    "print(combined_features.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
