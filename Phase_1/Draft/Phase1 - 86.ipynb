{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Libraries:\n",
    "# Using all of the channels\n",
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import welch, stft\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_participant_1 = 'Data/RawData/Participant_1.edf'\n",
    "file_participant_2 = 'Data/RawData/Participant_2.edf'\n",
    "file_participant_3 = 'Data/RawData/Participant_3.edf'\n",
    "file_participant_4 = 'Data/RawData/Participant_4.edf'\n",
    "file_participant_5 = 'Data/RawData/Participant_5.edf'\n",
    "file_participant_6 = 'Data/RawData/Participant_6.edf'\n",
    "file_participant_7 = 'Data/RawData/Participant_7.edf'\n",
    "file_participant_8 = 'Data/RawData/Participant_8.edf'\n",
    "file_participant_9 = 'Data/RawData/Participant_9.edf'\n",
    "file_participant_10 = 'Data/RawData/Participant_10.edf'\n",
    "file_participant_11 = 'Data/RawData/Participant_11.edf'\n",
    "file_participant_12 = 'Data/RawData/Participant_12.edf'\n",
    "file_participant_13 = 'Data/RawData/Participant_13.edf'\n",
    "\n",
    "edf_data_files = [\n",
    "    file_participant_1,\n",
    "    file_participant_2,\n",
    "    file_participant_3,\n",
    "    file_participant_4,\n",
    "    file_participant_5,\n",
    "    file_participant_6,\n",
    "    file_participant_7,\n",
    "    file_participant_8,\n",
    "    file_participant_9,\n",
    "    file_participant_10,\n",
    "    file_participant_11,\n",
    "    file_participant_12,\n",
    "    file_participant_13\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Channels of Interest:\n",
    "\n",
    "#channels_of_interest = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove start and finish and remove the breaks as well:\n",
    "\n",
    "# Loading the first file to use as a reference for channel names\n",
    "print(\"Loading reference EDF file...\")\n",
    "reference_raw = mne.io.read_raw_edf(edf_data_files[0], preload=True)\n",
    "reference_channels = reference_raw.info['ch_names']\n",
    "\n",
    "raw_objects = []\n",
    "\n",
    "# Define the segments of interest in seconds:\n",
    "segments = [\n",
    "    (30, 90),  # \"I\"\n",
    "    (120, 180),  # \"Yes\"\n",
    "    (210, 270),  # \"No\"\n",
    "    (300, 360),  # \"Want\"\n",
    "    (390, 450),  # \"Help\"\n",
    "    (480, 540),  # \"More\"\n",
    "    (570, 630),  # \"That\"\n",
    "    (660, 720),  # \"Stop\"\n",
    "    (750, 810),  # \"Open\"\n",
    "    (840, 900)   # \"Close\"\n",
    "]\n",
    "\n",
    "for file_path in edf_data_files:\n",
    "    print(f\"Loading and cropping EDF file: {file_path}...\")\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    raw.pick_channels(reference_channels)\n",
    "    \n",
    "    # Create an empty list to store the segments:\n",
    "    data_segments = []\n",
    "    \n",
    "    for start, end in segments:\n",
    "        segment = raw.copy().crop(tmin=start, tmax=end)\n",
    "        data_segments.append(segment)\n",
    "    \n",
    "    # Concatenate the segments:\n",
    "    raw_concatenated = mne.concatenate_raws(data_segments)\n",
    "    raw_objects.append(raw_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the resulting files after segmentation:\n",
    "\n",
    "raw_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all loaded and processed objects:\n",
    "\n",
    "print(\"Concatenating all processed EDF files...\")\n",
    "raw = mne.concatenate_raws(raw_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess Raw Data:\n",
    "\n",
    "def preprocess_raw_data(raw):\n",
    "\n",
    "    # Handling NaNs: Replace NaNs with the mean of the respective channel\n",
    "    raw_data = raw.get_data()\n",
    "\n",
    "    for i in range(raw_data.shape[0]):\n",
    "        nan_indices = np.isnan(raw_data[i])\n",
    "\n",
    "        if np.any(nan_indices):\n",
    "            mean_value = np.nanmean(raw_data[i])\n",
    "            raw_data[i, nan_indices] = mean_value\n",
    "\n",
    "    raw._data = raw_data\n",
    "\n",
    "    # Filtering: Bandpass filter between 0.5-30 Hz\n",
    "    raw.filter(0.5, 30., fir_design='firwin')\n",
    "    \n",
    "    # Artifact Removal: ICA\n",
    "    ica = mne.preprocessing.ICA(n_components = 14, random_state = 97, max_iter = 800)\n",
    "    ica.fit(raw)\n",
    "    raw = ica.apply(raw)\n",
    "    \n",
    "    # Spatial Filtering: Common Average Reference (CAR)\n",
    "    raw.set_eeg_reference('average', projection = True)\n",
    "    \n",
    "    # Channel Interpolation: Interpolate bad channels\n",
    "    raw.interpolate_bads()\n",
    "\n",
    "    # Baseline Correction: Apply baseline correction using the mean of the segment\n",
    "    raw.apply_function(lambda x: x - np.mean(x), picks = 'eeg')\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the Raw Data:\n",
    "\n",
    "raw = preprocess_raw_data(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Fixed-Length Epochs:\n",
    "\n",
    "epoch_duration = 60  # seconds\n",
    "start_times = np.arange(0, raw.times[-1] - epoch_duration, epoch_duration)\n",
    "end_times = start_times + epoch_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Times: \", start_times)\n",
    "print(\"\\nTotal Number of Start Times: \", len(start_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End Times: \", end_times)\n",
    "print(\"\\nTotal Number of End Times: \", len(end_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Words:\n",
    "\n",
    "words = ['I', 'Yes', 'No', 'Want', 'Help', 'More', 'That', 'Stop', 'Open', 'Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Frequency:\n",
    "\n",
    "sfreq = raw.info['sfreq']\n",
    "print(\"Frequency Across Channels: \", sfreq, \"Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function:\n",
    "\n",
    "def extract_features(epoch_data, sfreq):\n",
    "\n",
    "    # Calculating Statistical Features:\n",
    "    mean_vals = np.mean(epoch_data, axis = 1) # Mean value of the signal for each channel.\n",
    "    std_vals = np.std(epoch_data, axis = 1) # Standard deviation of the signal for each channel.\n",
    "    skew_vals = skew(epoch_data, axis = 1) # Skewness of the signal for each channel, indicating asymmetry.\n",
    "    kurt_vals = kurtosis(epoch_data, axis = 1) # Kurtosis of the signal for each channel, indicating peakedness.\n",
    "\n",
    "    # Power Spectral Density (PSD) Features:\n",
    "    freqs, psd = welch(epoch_data, sfreq, nperseg = int(sfreq)) # Computes the PSD using Welchâ€™s method.\n",
    "    # Average power in the theta (4-8 Hz), alpha (8-12 Hz), and beta (12-30 Hz) frequency bands:\n",
    "    theta_power = psd[:, (freqs > 4) & (freqs <= 8)].mean(axis = 1)\n",
    "    alpha_power = psd[:, (freqs > 8) & (freqs <= 12)].mean(axis = 1)\n",
    "    beta_power = psd[:, (freqs > 12) & (freqs <= 30)].mean(axis = 1)\n",
    "\n",
    "    # Short-Time Fourier Transform (STFT) Features:\n",
    "    _, _, Zxx = stft(epoch_data, fs = sfreq, nperseg = int(sfreq/2)) # Computes the STFT, which provides time-frequency representation of the signal.\n",
    "    stft_power = np.abs(Zxx).mean(axis = 2) # Mean power from the STFT representation, averaged over time.\n",
    "    \n",
    "    # Entropy Feature:\n",
    "    entropy_vals = np.array([entropy(np.abs(epoch_data[channel, :])) for channel in range(epoch_data.shape[0])]) # Computes the entropy of the signal for each channel, indicating the complexity or randomness of the signal.\n",
    "\n",
    "    # Combining Features:\n",
    "    features = np.stack([\n",
    "        mean_vals,\n",
    "        std_vals,\n",
    "        skew_vals,\n",
    "        kurt_vals,\n",
    "        theta_power,\n",
    "        alpha_power,\n",
    "        beta_power,\n",
    "        stft_power.mean(axis=1),\n",
    "        entropy_vals\n",
    "    ], axis = 1) # Combines all extracted features into a single array with each feature as a column.\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting the Data into Epochs and Sub-Epochs:\n",
    "\n",
    "print(\"Segmenting data into 60-second epochs, then into 2-second sub-epochs, and extracting features...\")\n",
    "\n",
    "labeled_features_data = []\n",
    "sub_epoch_duration = 2  # seconds\n",
    "\n",
    "for i, (start, end) in enumerate(zip(start_times, end_times)): # Loops through each 30-second epoch:\n",
    "    start_sample = int(start * sfreq)\n",
    "    end_sample = int(end * sfreq)\n",
    "    epoch_data, _ = raw[:, start_sample:end_sample] # Extracts the EEG data for the current epoch.\n",
    "    word_label = words[i % len(words)] # Assigns a label to the current epoch using a list of predefined words.\n",
    "    \n",
    "    for j in range(int(epoch_duration / sub_epoch_duration)): # Iterate over Sub-Epochs:\n",
    "        sub_start = j * sub_epoch_duration * int(sfreq)\n",
    "        sub_end = (j + 1) * sub_epoch_duration * int(sfreq)\n",
    "        sub_epoch_data = epoch_data[:, sub_start:sub_end]\n",
    "        # Calls the extract_features function to extract from sub-epoch:\n",
    "        features = extract_features(sub_epoch_data, sfreq)\n",
    "        labeled_features_data.append((features, word_label)) # Stores the extracted features along with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labeled_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features and Labels:\n",
    "\n",
    "#features, labels = zip(*labeled_features_data) if labeled_features_data else ([], [])\n",
    "#features = np.array(features) if features else np.empty((0, 0))\n",
    "#labels = np.array(labels)\n",
    "\n",
    "features = np.array([f[0] for f in labeled_features_data])\n",
    "labels = np.array([f[1] for f in labeled_features_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the last two dimensions of the features array.\n",
    "# To transform the 3D feature array into a 2D array where each row represents a single sample and each column represents a feature.\n",
    "features_2d = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Handling NaN values: replacing them with the column mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\n",
    "features_imputed = imputer.fit_transform(features_2d)\n",
    "\n",
    "# Scaling the features\n",
    "# To standardize the features by scaling them so that they have a mean of 0 and a standard deviation of 1.\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components = 0.95) \n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Mutual Information\n",
    "num_sub_epochs_per_epoch = int(epoch_duration / sub_epoch_duration)\n",
    "total_sub_epochs = num_sub_epochs_per_epoch * len(start_times)\n",
    "\n",
    "num_features = features_scaled.shape[1]\n",
    "k_best = min(num_features, 20)  # Ensure k does not exceed the number of available features\n",
    "mi_selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "features_mi = mi_selector.fit_transform(features_scaled, labels)\n",
    "\n",
    "# Choosing feature set to use for further model training\n",
    "selected_features = features_mi\n",
    "\n",
    "num_features_mi = features_mi.shape[1]  # Number of features after MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    selected_features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "val_features, test_features, val_labels, test_labels = train_test_split(\n",
    "    test_features, test_labels, test_size=0.5, random_state=42, stratify=test_labels)\n",
    "\n",
    "print(f\"Training data size: {len(train_features)}\")\n",
    "print(f\"Validation data size: {len(val_features)}\")\n",
    "print(f\"Testing data size: {len(test_features)}\")\n",
    "\n",
    "# Counting occurrences of each label in the training, validation, and testing sets\n",
    "train_label_counts = Counter(train_labels)\n",
    "val_label_counts = Counter(val_labels)\n",
    "test_label_counts = Counter(test_labels)\n",
    "\n",
    "# Calculating the total number of samples in each set\n",
    "total_train = len(train_labels)\n",
    "total_val = len(val_labels)\n",
    "total_test = len(test_labels)\n",
    "\n",
    "# Printing the distribution of each label in each set\n",
    "print(\"Training set label distribution:\")\n",
    "for label, count in train_label_counts.items():\n",
    "    print(f\"{label}: {count} ({count / total_train * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nValidation set label distribution:\")\n",
    "for label, count in val_label_counts.items():\n",
    "    print(f\"{label}: {count} ({count / total_val * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTesting set label distribution:\")\n",
    "for label, count in test_label_counts.items():\n",
    "    print(f\"{label}: {count} ({count / total_test * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class EEGTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=256, nhead=8, num_layers=16, dim_feedforward=512, dropout_rate=0.1, noise_std=0.01):\n",
    "        super(EEGTransformerEncoder, self).__init__()\n",
    "        self.noise_std = noise_std\n",
    "        self.linear_in = nn.Linear(input_dim, d_model)\n",
    "        self.dropout_in = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Using pre-LayerNorm\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward, \n",
    "                                                dropout=dropout_rate, \n",
    "                                                activation='gelu', \n",
    "                                                norm_first=True)\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.linear_out = nn.Linear(d_model, num_classes)\n",
    "        self.dropout_out = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.noise_std > 0.0:\n",
    "            noise = torch.randn_like(x) * self.noise_std\n",
    "            x = x + noise\n",
    "\n",
    "        x = self.linear_in(x)\n",
    "        x = self.dropout_in(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x)  # Transformer encoder with pre-LayerNorm\n",
    "\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout_out(x)\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = EEGTransformerEncoder(input_dim=num_features_mi, num_classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "# Encoding string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "print(test_labels)\n",
    "print(test_labels_encoded)\n",
    "    \n",
    "train_dataset = EEGDataset(train_features, train_labels_encoded)\n",
    "val_dataset = EEGDataset(val_features, val_labels_encoded)\n",
    "test_dataset = EEGDataset(test_features, test_labels_encoded)\n",
    "\n",
    "# Defining DataLoader\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235, Loss: 0.08224101852584671, Training Accuracy: 0.8600732600732601, Validation Loss: 0.0016875428521735036, Validation Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 600\n",
    "\n",
    "# Using CrossEntropyLoss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-8)\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "l1_lambda = 0.0001\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels)\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += l1_lambda * l1_norm  # L1 regularization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = torch.max(output, 1)[1]\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = train_correct / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.max(output, 1)[1]\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / total_val\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / total_train}, Training Accuracy: {train_accuracy}, '\n",
    "          f'Validation Loss: {val_loss / total_val}, Validation Accuracy: {val_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
