{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse Click with Left Eye:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks = True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = screen_w * landmark.x\n",
    "                screen_y = screen_h * landmark.y\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        if (left[0].y - left[1].y) < 0.004:\n",
    "            pyautogui.click()\n",
    "            pyautogui.sleep(1)\n",
    "\n",
    "    cv2.imshow('Eye Controlled Mouse', frame)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a screenshot of the whole screen.\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize face mesh\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    objects = []\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            objects.append(label)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame, objects\n",
    "\n",
    "detection_done = False\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = screen_w * landmark.x\n",
    "                screen_y = screen_h * landmark.y\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        if (left[0].y - left[1].y) < 0.004 and not detection_done:\n",
    "            pyautogui.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Capture screenshot\n",
    "            screenshot = pyautogui.screenshot()\n",
    "            screenshot = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            frame, objects = detect_objects(screenshot)\n",
    "            if objects:\n",
    "                print(\"Objects detected:\", \", \".join(objects))\n",
    "                detection_done = True\n",
    "\n",
    "    cv2.imshow('Eye Controlled Mouse', frame)\n",
    "    if detection_done or cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize face mesh\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    print(f\"Frame size for detection: width={width}, height={height}, channels={channels}\")\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.3:  # Lowering the confidence threshold for debugging\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.3, 0.4)  # Lowering the threshold for NMS\n",
    "    objects = []\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            print(f\"Detected {label} with confidence {confidences[i]} at [{x}, {y}, {w}, {h}]\")\n",
    "            objects.append(label)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame, objects\n",
    "\n",
    "detection_done = False\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        if (left[0].y - left[1].y) < 0.004 and not detection_done:\n",
    "            pyautogui.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Capture a specific region around the click location\n",
    "            region_size = 400  # Define the size of the region to capture\n",
    "            left = max(0, screen_x - region_size // 2)\n",
    "            top = max(0, screen_y - region_size // 2)\n",
    "            width = min(region_size, screen_w - left)\n",
    "            height = min(region_size, screen_h - top)\n",
    "            print(f\"Capturing region: left={left}, top={top}, width={width}, height={height}\")\n",
    "            screenshot = pyautogui.screenshot(region=(left, top, width, height))\n",
    "            screenshot = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            print(\"Running object detection on the captured region...\")\n",
    "            frame, objects = detect_objects(screenshot)\n",
    "            if objects:\n",
    "                print(\"Objects detected:\", \", \".join(objects))\n",
    "                detection_done = True\n",
    "                # Display the detection results\n",
    "                cv2.imshow('Detected Objects', frame)\n",
    "                cv2.waitKey(0)  # Wait for any key to be pressed to close the window\n",
    "                break  # Exit the while loop after detection\n",
    "            else:\n",
    "                print(\"No objects detected.\")\n",
    "\n",
    "    cv2.imshow('Eye Controlled Mouse', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works v2:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize face mesh\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    print(f\"Frame size for detection: width={width}, height={height}, channels={channels}\")\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    highest_confidence = 0\n",
    "    best_object = None\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.3:  # Lowering the confidence threshold for debugging\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.3, 0.4)  # Lowering the threshold for NMS\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            if confidences[i] > highest_confidence:\n",
    "                highest_confidence = confidences[i]\n",
    "                best_object = (classes[class_ids[i]], boxes[i])\n",
    "\n",
    "    if best_object:\n",
    "        label, (x, y, w, h) = best_object\n",
    "        print(f\"Detected {label} with confidence {highest_confidence} at [{x}, {y}, {w}, {h}]\")\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        return frame, label\n",
    "    else:\n",
    "        return frame, None\n",
    "\n",
    "detection_done = False\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        if (left[0].y - left[1].y) < 0.004 and not detection_done:\n",
    "            pyautogui.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Capture a specific region around the click location\n",
    "            region_size = 400  # Define the size of the region to capture\n",
    "            left = max(0, screen_x - region_size // 2)\n",
    "            top = max(0, screen_y - region_size // 2)\n",
    "            width = min(region_size, screen_w - left)\n",
    "            height = min(region_size, screen_h - top)\n",
    "            print(f\"Capturing region: left={left}, top={top}, width={width}, height={height}\")\n",
    "            screenshot = pyautogui.screenshot(region=(left, top, width, height))\n",
    "            screenshot = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            print(\"Running object detection on the captured region...\")\n",
    "            frame, best_object = detect_objects(screenshot)\n",
    "            if best_object:\n",
    "                print(\"Best object detected:\", best_object)\n",
    "                detection_done = True\n",
    "                # Display the detection results\n",
    "                cv2.imshow('Detected Objects', frame)\n",
    "                cv2.waitKey(0)  # Wait for any key to be pressed to close the window\n",
    "                break  # Exit the while loop after detection\n",
    "            else:\n",
    "                print(\"No objects detected.\")\n",
    "\n",
    "    cv2.imshow('Eye Controlled Mouse', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Version Yet:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize face mesh\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "    print(f\"Frame size for detection: width={width}, height={height}, channels={channels}\")\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    highest_confidence = 0\n",
    "    best_object = None\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.3:  # Lowering the confidence threshold for debugging\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.3, 0.4)  # Lowering the threshold for NMS\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            if confidences[i] > highest_confidence:\n",
    "                highest_confidence = confidences[i]\n",
    "                best_object = (classes[class_ids[i]], boxes[i])\n",
    "\n",
    "    if best_object:\n",
    "        label, (x, y, w, h) = best_object\n",
    "        print(f\"Detected {label} with confidence {highest_confidence} at [{x}, {y}, {w}, {h}]\")\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        return frame, label\n",
    "    else:\n",
    "        return frame, None\n",
    "\n",
    "detection_done = False\n",
    "detected_object = None\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        if (left[0].y - left[1].y) < 0.004 and not detection_done:\n",
    "            pyautogui.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Capture a specific region around the click location\n",
    "            region_size = 400  # Define the size of the region to capture\n",
    "            left = max(0, screen_x - region_size // 2)\n",
    "            top = max(0, screen_y - region_size // 2)\n",
    "            width = min(region_size, screen_w - left)\n",
    "            height = min(region_size, screen_h - top)\n",
    "            print(f\"Capturing region: left={left}, top={top}, width={width}, height={height}\")\n",
    "            screenshot = pyautogui.screenshot(region=(left, top, width, height))\n",
    "            screenshot = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            print(\"Running object detection on the captured region...\")\n",
    "            frame, best_object = detect_objects(screenshot)\n",
    "            if best_object:\n",
    "                print(\"Best object detected:\", best_object)\n",
    "                detected_object = best_object\n",
    "                detection_done = True\n",
    "                # Display the detection results\n",
    "                cv2.imshow('Detected Objects', frame)\n",
    "                cv2.waitKey(0)  # Wait for any key to be pressed to close the window\n",
    "                break  # Exit the while loop after detection\n",
    "            else:\n",
    "                print(\"No objects detected.\")\n",
    "\n",
    "    cv2.imshow('Eye Controlled Mouse', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if detected_object:\n",
    "    print(f\"Detected object: {detected_object}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salib\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing region: left=935, top=196, width=400, height=400\n",
      "Running object detection on the captured region...\n",
      "Frame size for detection: width=400, height=400, channels=3\n",
      "No objects detected.\n",
      "Capturing region: left=904, top=293, width=400, height=400\n",
      "Running object detection on the captured region...\n",
      "Frame size for detection: width=400, height=400, channels=3\n",
      "Detected banana with confidence 0.9961012601852417 at [13, 237, 200, 164]\n",
      "Best object detected: banana\n",
      "Detected object: banana\n",
      "Using detected object: banana as a parameter in a transformer model.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
