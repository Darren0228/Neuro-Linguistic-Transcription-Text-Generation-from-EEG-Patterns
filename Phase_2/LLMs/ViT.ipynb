{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salib\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\salib\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing region: left = 847, top = 241, width = 600, height = 600\n",
      "Running object detection on the captured region...\n",
      "Detected web site with confidence 0.19114316999912262\n",
      "No objects detected.\n"
     ]
    }
   ],
   "source": [
    "# Successfull Implementation - Takes only the first category name + Both Eyes \n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image  \n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Load ViT model and Feature Extractor:\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Function to detect objects using ViT:\n",
    "def detect_objects(frame):\n",
    "    # Convert frame to PIL image:\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")  # Preprocess the image.\n",
    "\n",
    "    # Get predictions:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class = model.config.id2label[predicted_class_idx]\n",
    "    confidence = torch.softmax(logits, dim=1)[0, predicted_class_idx].item()\n",
    "\n",
    "    # Take only the first term of the predicted class:\n",
    "    first_term = predicted_class.split(',')[0].strip()  # Split and strip whitespace\n",
    "    \n",
    "    print(f\"Detected {first_term} with confidence {confidence}\")\n",
    "    \n",
    "    # Add a bounding box and label (for illustration):\n",
    "    cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"{first_term} ({confidence:.2f})\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return frame, first_term if confidence > 0.5 else None\n",
    "\n",
    "\n",
    "detection_done = False\n",
    "detected_object = None\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        # Left eye landmarks\n",
    "        left_eye = [landmarks[145], landmarks[159]]\n",
    "        # Right eye landmarks\n",
    "        right_eye = [landmarks[374], landmarks[386]]\n",
    "\n",
    "        for landmark in left_eye + right_eye:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        # Check for blinking on both eyes\n",
    "        left_blink = (left_eye[0].y - left_eye[1].y) < 0.020\n",
    "        right_blink = (right_eye[0].y - right_eye[1].y) < 0.020\n",
    "\n",
    "        if (left_blink or right_blink) and not detection_done:\n",
    "            pyautogui.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Capture a specific region around the click location:\n",
    "            region_size = 600  # Size of the region to capture.\n",
    "            left = max(0, screen_x - region_size // 2)\n",
    "            top = max(0, screen_y - region_size // 2)\n",
    "            width = min(region_size, screen_w - left)\n",
    "            height = min(region_size, screen_h - top)\n",
    "            print(f\"Capturing region: left = {left}, top = {top}, width = {width}, height = {height}\")\n",
    "            screenshot = pyautogui.screenshot(region=(left, top, width, height))\n",
    "            screenshot = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            print(\"Running object detection on the captured region...\")\n",
    "            frame, best_object = detect_objects(screenshot)\n",
    "            if best_object:\n",
    "                print(\"Best object detected:\", best_object)\n",
    "                detected_object = best_object\n",
    "                detection_done = True\n",
    "                # Display the detection results for a short period:\n",
    "                cv2.imshow('Detected Objects', frame)\n",
    "                cv2.waitKey(10000)  # Display the window for 10 seconds.\n",
    "                break  # Exit the while loop after detection.\n",
    "            else:\n",
    "                print(\"No objects detected.\")\n",
    "\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if detected_object:\n",
    "    print(f\"Detected object: {detected_object}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
