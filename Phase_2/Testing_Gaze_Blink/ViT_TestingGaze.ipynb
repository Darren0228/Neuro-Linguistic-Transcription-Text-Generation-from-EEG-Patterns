{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation 1:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image  \n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Load ViT model and Feature Extractor:\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Function to detect objects using ViT:\n",
    "def detect_objects(frame):\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")  # Preprocess the image.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_class = model.config.id2label[predicted_class_idx]\n",
    "    confidence = torch.softmax(logits, dim=1)[0, predicted_class_idx].item()\n",
    "\n",
    "    first_term = predicted_class.split(',')[0].strip()  # Split and strip whitespace\n",
    "    \n",
    "    print(f\"Detected {first_term} with confidence {confidence}\")\n",
    "    \n",
    "    cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"{first_term} ({confidence:.2f})\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return frame, first_term if confidence > 0.5 else None\n",
    "\n",
    "detection_done = False\n",
    "detected_object = None\n",
    "\n",
    "# Variables to track accuracy\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        if (left[0].y - left[1].y) < 0.004 and not detection_done:\n",
    "            pyautogui.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            region_size = 600  # Size of the region to capture.\n",
    "            left = max(0, screen_x - region_size // 2)\n",
    "            top = max(0, screen_y - region_size // 2)\n",
    "            width = min(region_size, screen_w - left)\n",
    "            height = min(region_size, screen_h - top)\n",
    "            screenshot = pyautogui.screenshot(region=(left, top, width, height))\n",
    "            screenshot = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            frame, best_object = detect_objects(screenshot)\n",
    "            if best_object:\n",
    "                detected_object = best_object\n",
    "                detection_done = True\n",
    "                # Display the detection results for a short period:\n",
    "                cv2.imshow('Detected Objects', frame)\n",
    "                cv2.waitKey(10000)  # Display the window for 10 seconds.\n",
    "                break  # Exit the while loop after detection.\n",
    "            else:\n",
    "                print(\"No objects detected.\")\n",
    "\n",
    "        # Calculate accuracy by comparing predicted and actual screen coordinates\n",
    "        if abs(screen_x - pyautogui.position().x) < 20 and abs(screen_y - pyautogui.position().y) < 20:\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "        accuracy = (correct_predictions / total_samples) * 100\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if detected_object:\n",
    "    print(f\"Detected object: {detected_object}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 100.00%\n",
      "Accuracy Range: 100.00% - 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Controlled Environment:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Variables to track accuracy\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "accuracy_values = []\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        # Calculate accuracy by comparing predicted and actual screen coordinates\n",
    "        if abs(screen_x - pyautogui.position().x) < 20 and abs(screen_y - pyautogui.position().y) < 20:\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "        accuracy = (correct_predictions / total_samples) * 100\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "    cv2.imshow('Gaze Detection Accuracy', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate and display final results\n",
    "if accuracy_values:\n",
    "    average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    min_accuracy = min(accuracy_values)\n",
    "    max_accuracy = max(accuracy_values)\n",
    "    print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Range: {min_accuracy:.2f}% - {max_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 100.00%\n",
      "Accuracy Range: 100.00% - 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Dim Light:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Variables to track accuracy\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "accuracy_values = []\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        # Calculate accuracy by comparing predicted and actual screen coordinates\n",
    "        if abs(screen_x - pyautogui.position().x) < 20 and abs(screen_y - pyautogui.position().y) < 20:\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "        accuracy = (correct_predictions / total_samples) * 100\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "    cv2.imshow('Gaze Detection Accuracy', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate and display final results\n",
    "if accuracy_values:\n",
    "    average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    min_accuracy = min(accuracy_values)\n",
    "    max_accuracy = max(accuracy_values)\n",
    "    print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Range: {min_accuracy:.2f}% - {max_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 99.35%\n",
      "Accuracy Range: 98.15% - 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Bright Light:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Variables to track accuracy\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "accuracy_values = []\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        # Calculate accuracy by comparing predicted and actual screen coordinates\n",
    "        if abs(screen_x - pyautogui.position().x) < 20 and abs(screen_y - pyautogui.position().y) < 20:\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "        accuracy = (correct_predictions / total_samples) * 100\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "    cv2.imshow('Gaze Detection Accuracy', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate and display final results\n",
    "if accuracy_values:\n",
    "    average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    min_accuracy = min(accuracy_values)\n",
    "    max_accuracy = max(accuracy_values)\n",
    "    print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Range: {min_accuracy:.2f}% - {max_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 99.20%\n",
      "Accuracy Range: 95.48% - 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Artificial Light:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Variables to track accuracy\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "accuracy_values = []\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        # Calculate accuracy by comparing predicted and actual screen coordinates\n",
    "        if abs(screen_x - pyautogui.position().x) < 20 and abs(screen_y - pyautogui.position().y) < 20:\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "        accuracy = (correct_predictions / total_samples) * 100\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "    cv2.imshow('Gaze Detection Accuracy', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate and display final results\n",
    "if accuracy_values:\n",
    "    average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    min_accuracy = min(accuracy_values)\n",
    "    max_accuracy = max(accuracy_values)\n",
    "    print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Range: {min_accuracy:.2f}% - {max_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 100.00%\n",
      "Accuracy Range: 100.00% - 100.00%\n"
     ]
    }
   ],
   "source": [
    "# With Glasses:\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize face mesh:\n",
    "cam = cv2.VideoCapture(0)\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "screen_w, screen_h = pyautogui.size()\n",
    "\n",
    "# Variables to track accuracy\n",
    "total_samples = 0\n",
    "correct_predictions = 0\n",
    "accuracy_values = []\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = face_mesh.process(rgb_frame)\n",
    "    landmark_points = output.multi_face_landmarks\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    if landmark_points:\n",
    "        landmarks = landmark_points[0].landmark\n",
    "\n",
    "        for id, landmark in enumerate(landmarks[474:478]):\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 0))\n",
    "\n",
    "            if id == 1:\n",
    "                screen_x = int(screen_w * landmark.x)\n",
    "                screen_y = int(screen_h * landmark.y)\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        left = [landmarks[145], landmarks[159]]\n",
    "\n",
    "        for landmark in left:\n",
    "            x = int(landmark.x * frame_w)\n",
    "            y = int(landmark.y * frame_h)\n",
    "            cv2.circle(frame, (x, y), 3, (0, 255, 255))\n",
    "\n",
    "        # Calculate accuracy by comparing predicted and actual screen coordinates\n",
    "        if abs(screen_x - pyautogui.position().x) < 20 and abs(screen_y - pyautogui.position().y) < 20:\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "        accuracy = (correct_predictions / total_samples) * 100\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "    cv2.imshow('Gaze Detection Accuracy', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate and display final results\n",
    "if accuracy_values:\n",
    "    average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    min_accuracy = min(accuracy_values)\n",
    "    max_accuracy = max(accuracy_values)\n",
    "    print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy Range: {min_accuracy:.2f}% - {max_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
